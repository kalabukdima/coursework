В качестве базовой оценки качества исследуемых алгоритмов предлагалось
использовать точность основных алгоритмов машинного обучения, выполняющих
классификацию изображений. В качестве таких алгоритмов использовались SVM, KNN,
Random Forest. Как и большинство алгоритмов классификации, они требуют, чтобы их
входом были точки (объекты) в $M$-мерном пространстве, а выходом является
число --- номер класса, или по одному числу в интервале $[0, 1]$ на каждый
класс --- вероятности того, что объект лежит в данном классе. Поэтому из каждого
исходного изображения были вырезаны квадраты $64 \times 64$ пикселя, в которых
больше $90\%$ площади принадлежат одному классу.  Классом изображения считается
преобладающий класс пикселей. Для представления таких изображений в виде точек
(векторов признаков) использовались различные дескрипторы.


\section{Классические алгоритмы}
Рассмотрим использованные алгоритмы машинного обучения подробнее.

\subsection{Linear SVM}
В случае двух классов предполагает, что выборка линейно разделима, т. е.
существует гиперплоскость, разделяющая всё пространство $\mathbb{R}^M$ на два
полупространства таких, что объекты, принадлежащие разным классам, лежат в
разных полупространствах. При этом из всех возможных разделяющих гиперплоскостей
выбирается та, которая наиболее удалена от объектов обоих классов.  В случае
нескольких классов рассматривается каждая пара классов, определяется
принадлежность классу из пары, а затем производится усреднение результатов для
получения предсказания номера класса

\subsection{K Nearest Neighbors}
Для определения класса данного объекта рассматриваются $K$ объектов из обучающей
выборки (для которой известны ответы), находящиеся ближе всего к данному объекту
по какой-то заранее заданной метрике в пространстве $\mathbb{R}^M$. В качестве
ответа для объекта используется класс, к которому принадлежит большинство из
найденных ``соседей''. В данной работе использовался параметр K равный $1$,
потому что он показывал лучший результат при экспериментах.

\subsection{Random Forest}
Использует голосование решающих деревьев. Каждое дерево в каждой своей вершине
пытается разделить множество объектов по, так называемому, решающему правилу,
которое использует какое-то небольшое подмножество признаков. При помощи
различных эвристик решающее правило выбирается так, чтобы разделять классы как
можно более равномерно. Полученные подмножества разделяются более глубокими
вершинами дерева. В листьях хранится ответ. Как правило, это самая частая метка
класса из множества объектов обучающей выборки, попавших в этот лист.
Преимуществами является способность классифицировать линейно неразделимую
выборку. Для всех экспериментов использовалось $75$ решающих деревьев, т.к. это
значение давало довольно хороший результат при приемлимом времени обучения.


\section{Дескрипторы}
Для преобразования изображений $64 \times 64$ пикселя в точки в $\mathbb{R}^M$
использовались следующие дескрипторы.

\subsection{Гистограмма яркостей}
Сначала изображение преобразуется в чёрно-белое. Затем спектр яркостей
$[0, 255]$ делится на $16$ равных отрезков: $[0, 15], [16, 31], \ldots, [240,
255]$. Для каждого отрезка вычисляется количество пикселей изображения, яркость
которых находится в соответствующем этому отрезку диапазоне. Полученные $16$ чисел
являются признаками, т. е. координатами точки, соответствующей данному
изображению.

\subsection{Гистограмма независимых цветов}
Указанные выше $16$ признаков считаются для каждого из трёх каналов изображения
(каждый из которых можно представлять как чёрно-белое изображение) и
конкатенируются. Недостатком такого подхода является то, что каналы изображения
считаются независимыми, а значит преобразование одного из каналов, которое не
изменит дескриптор, может сильно изменить цвета изображения. Очевидно, что такой
подход даёт плохие результаты, и почти не используется на практике.

\subsection{Кластеризация цветов}
Цвета представляются как точки в трёхмерном пространстве. Выбирается количество
кластеров $K$. В данной работе проводились эксперименты с количеством кластеров
32, 64 и 128. Все цвета, встречающиеся во всех изображениях разделяются на
выбранное количество кластеров алгоритмом K-Means, запоминаются центроиды
(центры масс) каждого кластера. Затем для данного изображения и каждого номера
кластера подсчитывается, сколько пикселей относятся к этому кластеру. Считается,
что пиксель относится к кластеру, если он удалён от центроида этого кластера
меньше, чем от центроидов всех остальных кластеров. Получается $K$ признаков для
каждого изображения. Т.к. количество пикселей на всех изображениях очень
большое, для кластеризации используется случайное их подмножество.


\section{Метрики}
Во всех экспериментах в качестве метрики использовалась \textit{точ\-ность}
(англ. \textit{ac\-cu\-ra\-cy}) --- доля правильно предсказанных изображений из всех. У
такой метрики есть недостатки. Например, иногда на практике ошибки в одних
классах более критичные, чем другие. Также при большом количестве классов
алгоритм может вовсе не научиться определять один из них, но при этом иметь
хорошую точность засчёт корректного определения других. Для наших целей эта
метрика была достаточно показательной.


\section{Результаты классических алгоритмов}
Для сравнения алгоритмов и дескрипторов была измерена точность классификации
изображений. Алгоритмы обучались на некоторых размеченных данных. Затем
проводились измерения качества на данных того же поставщика. В таблице
\ref{table:classical_algo__comparison} приведены полученные значения точности.

\begin{table}[ht]
\caption{Сравнение классических алгоритмов и дескрипторов}
\label{table:classical_algo__comparison}
\small
\begin{tabular}{|m{68mm}|m{26mm}|m{26mm}|m{28mm}|}
    \hline
    & LinearSVM & KNN & RandomForest\\
    \hline
    Гистограмма яркостей           & $0.851$ & $0.883$ & $0.920$\\
    \hline
    Гистограмма независимых цветов & $0.879$ & $0.903$ & $0.927$\\
    \hline
    Кластеризация, $32$ кластера   & $0.906$ & $0.900$ & $0.940$\\
    \hline
    Кластеризация, $64$ кластера   & $0.883$ & $0.902$ & $0.940$\\
    \hline
    Кластеризация, $128$ кластера  & $0.896$ & $0.898$ & $0.934$\\
    \hline
\end{tabular}
\end{table}

Также были проведены эксперименты по обучению и проверке качества на различных
поставщиках. Результаты (значения точности) приведены в таблице
\ref{table:providers_comparison}.

\begin{table}[ht]
\caption{Результаты при обучении на данных разных поставщиков}
\label{table:providers_comparison}
\small
\begin{tabular}{|r|r|m{22mm}m{22mm}m{22mm}m{22mm}|}
    \hline
         &        & \multicolumn{4}{|c|}{Поставщик для оценки качества}\\
    \hline
         &        & Google  & Yandex  & Bing    & ESRI\\
    \hline
    \multirow{4}{24mm}{Поставщик для обучения}
         & Google & --      & $0.819$ & $0.956$ & $0.840$\\
         & Yandex & $0.432$ & --      & $0.843$ & $0.861$\\
         & Bing   & $0.754$ & $0.915$ & --      & $0.960$\\
         & ESRI   & $0.593$ & $0.908$ & $0.988$ & --\\
    \hline
\end{tabular}
\end{table}

В результате этого эксперимента стало понятно, что между изображениями различных
поставщиков существуют значительные различия, что заметно влияет на качество
классификации. В дальнейшем использовались разные подходы для устранения этой
проблемы.


\section{Нейросетевые алгоритмы}
Нейросетевыми называются такие вычислительные системы, которые обладают
способностью к самообучению и постепенному повышению производительности. Они
используются при решении таких задач, которые не поддаются логическому
программированию. Таковыми являются многие задачи анализа и обработки
изображений, в том числе задача классификации изображений.
В задачах связанных с изображениями чаще всего применяются \textit{свёрточные
нейронные сети} (англ. \textit{convolutional neural networks}). В основе таких
сетей лежит операция свёртки матриц. Такие сети позволяют использовать меньше
нейронов по сравнению с сетями использующими полносвязные слои, и обладают
свойством локальности --- результаты работы сети слабо подвержены таким
изменениям как перемещение целевого объекта на изображении.

Одной из известных архитектур свёрточной нейронной сети для классификации
является архитекрута VGG. Основное её отличие от других свёрточных сетей ---
замена слоёв с фильтрами $n \times n$ на комбинацию слоёв $3 \times 3$. Такой
подход даёт примерно такие же возможности классификации, но задействуется меньше
обучаемых весов, что способствует более быстрому обучению и лучшему качеству
сети. Например, фильтр $5 \times 5$ использует $25$ связей (и, соответственно,
весов), а два слоя $3 \times 3$ обладают тем же рецептивным полем, но суммарно
используют $18$ связей.
Рецептивное поле --- окрестность в предыдущем слое
нейросети, значения которой влияют на значение фиксированного нейрона в текущем
слое.  Ещё одной особенностью архитектуры VGG является увеличение числа
свёрточных фильтров в $2$ раза по сравнению с предыдущим слоем.

Использованная нейронная сеть состоит из $5$ свёрточных уровней и двух
полносвязных слоёв. Каждый уровень состоит из двух свёрточных слоёв с фильтрами
$3 \times 3$, слоя активации (ReLU) и MaxPooling слоя. MaxPooling --- это слой,
уменьшащий размер карты признаков путём замены каждой окрестности $2 \times 2$
пикселя на $1$ число --- максимум чисел в этой окрестности.

Для работы с многоклассовыми данными используется one-hot-encoding --- метки
классов для каждого пикселя в ответе заменяются на вектор, длина которого
равна количеству классов. В этом векторе на позиции, соответствующей номеру
класса, стоит $1$, а на остальных --- $0$. Последний полносвязный слой сети
состоит из числа нейронов равного числу классов изобажений. Результатом работы
алгоритма на изображении является вектор, длина которого равна числу классов,
--- вероятности классов (степени уверенности алгоритма в принадлежности
изображения данному классу). 

Для решения проблемы вариативности данных была использована \textit{аугментация}
(англ.  \textit{augmentation}) --- небольшое случайное изменение входных
изображений во время обучения так, чтобы класс, которому принадлежит изображение
остался прежним. Аугментация позволяет имитировать вариативность выборки, не
проделывая дополнительную работу по подготовке данных. Такой приём повысил
точность с $78\%$ до $82\%$.
